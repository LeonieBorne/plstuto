{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reduction\n",
    "\n",
    "Like many other machine learning methods, canonical correlation analysis (CCA) is intended to be used when the number of data points (sample size) exceeds the number of features by a healthy amount. However, in neuroimaging, the number of features (e.g. edges in a functional connectome) is often far greater than the sample size. One solution to this is using the sparse version of CCA (sCCA), which strives to find parsimonious combinations of features, but there are limits to sCCA's ability to select features.\n",
    "\n",
    "In such situation, it is appropriate to reduce the dimensionality of the input data to a manageable scale before applying CCA. There are two types of dimension reduction methods: feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection\n",
    "\n",
    "The first group of methods selects features to input based on whether they have sufficient variation statistically. Because we are *completely excluding* the unselected features from analysis, it is important to exercise care in how we do this selection.\n",
    "\n",
    "One naive solution is selecting features with highest standard deviations. While this can suffice in some situations, standard deviation is known to be susceptible to outliers. A more robust metric is [mean or median absolute deviation](https://en.wikipedia.org/wiki/Average_absolute_deviation) (MAD), which is the mean/median of absolute differences to the feature's mean/median.\n",
    "\n",
    "For example, Xia et al. (2018) used median absolute deviation to take advantage of median function's robustness to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Xia 2018 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ANOVA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "The second group of methods uses other unsupervised learning methods to compress the original feature set into a smaller one.\n",
    "\n",
    "The most popular method for this is [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA), which strives to find uncorrelated linear combinations of features (components) that explain the most variance in the dataset. By selecting the components with the highest values of variance explained, we can create a new input dataset that is smaller than the original but retains most of the variance from it.\n",
    "\n",
    "Smith et al. (2015) used PCA to reduce the dimensions of both the functional connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Smith 2015 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO post processing with ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection vs Extraction\n",
    "\n",
    "The main advantage of feature selection is that the reduced dataset directly corresponds to original features from the data. Thus, the CCA results can be easily interpreted. However, this comes at the cost of excluding potentially meaningful features from the analysis completely. To mitigate this problem, Xia et al. (2018) repeated their analysis with PCA components to confirm that their original analysis is robust to the choice of dimension reduction method.\n",
    "\n",
    "In contrast, methods like PCA uses the whole dataset and do not make any assumptions about which features are the most meaningful. However, feature extraction methods may require new assumptions about the original dataset. For example, one assumption of PCA is that there are linear correlations between original features. Another challenge with feature extraction is interpreting the correlations between combinations of principal components after CCA. Solutions to this include using ICA to re-map the CCA solutions to the original feature set (Miller et al. 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:brainhack]",
   "language": "python",
   "name": "conda-env-brainhack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
